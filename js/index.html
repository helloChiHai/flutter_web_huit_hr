<!DOCTYPE html>
<html>
<head>
  <!--
    If you are serving your web app in a path other than the root, change the
    href value below to reflect the base path you are serving from.

    The path provided below has to start and end with a slash "/" in order for
    it to work correctly.

    For more details:
    * https://developer.mozilla.org/en-US/docs/Web/HTML/Element/base

    This is a placeholder for base href that will be replaced by the value of
    the `--base-href` argument provided to `flutter build`.
  -->
  <base href="/">

  <meta charset="UTF-8">
  <meta content="IE=Edge" http-equiv="X-UA-Compatible">
  <meta name="description" content="A new Flutter project.">

  <!-- iOS meta tags & icons -->
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black">
  <meta name="apple-mobile-web-app-title" content="jeenew_logging">
  <link rel="apple-touch-icon" href="icons/Icon-192.png">

  <!-- Favicon -->
  <link rel="icon" type="image/png" href="favicon.png"/>

  <title>jeenew_logging</title>
  <link rel="manifest" href="manifest.json">
  <script src="js/js_library.js"></script>
  <script src="js/face-api.min.js"></script>
  <script src="js/face-api.js"></script>
  <script>
    async function loadFaceModels() {
      await faceapi.nets.ssdMobilenetv1.loadFromUri('/models');
      await faceapi.nets.faceLandmark68Net.loadFromUri('/models');
      await faceapi.nets.faceRecognitionNet.loadFromUri('/models');
      await faceapi.nets.faceExpressionNet.loadFromUri('/models');
      console.log('Face-api models loaded');
    }
    console.log("Loading model from", '/models');
  
    async function detectFaceFromBase64(base64Image) {
      const img = new Image();
      img.src = base64Image;
  
      return new Promise((resolve) => {
        img.onload = async () => {
          const canvas = document.createElement('canvas');
          canvas.width = img.width;
          canvas.height = img.height;
          const ctx = canvas.getContext('2d');
          ctx.drawImage(img, 0, 0);
  
          const detections = await faceapi
          .detectAllFaces(canvas)
          .withFaceLandmarks()
          .withFaceDescriptors()
          .withFaceExpressions();
          let result = {
          faceCount: 0,
          isSmiling: false,
          // isEyesClosed: false,
          isTurnLeft: false,
          isTurnRight: false,
        };

        if (detections.length > 0) {
          result.faceCount = detections.length;

          const detection = detections[0];
          const expressions = detection.expressions;
          result.isSmiling = expressions.happy > 0.7;
          const landmarks = detection.landmarks;

          // const leftEye = landmarks.getLeftEye();
          // const rightEye = landmarks.getRightEye();

          // const leftEyeOpen = leftEye[4].y - leftEye[1].y > 2;
          // const rightEyeOpen = rightEye[4].y - rightEye[1].y > 2;
          // result.isEyesClosed = !leftEyeOpen && !rightEyeOpen;

          const nose = landmarks.getNose();
          const noseX = nose[3].x;
          const midX = canvas.width / 2;

          if (noseX < midX - 30) result.isTurnLeft = true;
          if (noseX > midX + 30) result.isTurnRight = true;
        }

        resolve(result);
        };
      });
    }
  
    window.loadFaceModels = loadFaceModels;
    window.detectFaceFromBase64 = detectFaceFromBase64;
  </script>
</head>
<body>
  <script src="flutter_bootstrap.js" async></script>
</body>
</html>
